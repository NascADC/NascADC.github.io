<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TSI</title>
</head>
<body>
    <reader>
    <h1>STATISTICAL INFERENCE</h1>
    <h2>Exact results, Large sample results, Parametric model, Non-parametric model, Regression model</h2>
    </reader>
    <main>
        ......................................................................... 
        &#8658; TOPICS OF THIS COURSE    
        .........................................................................
            <ol start="1">
            <li><section> 
              <h3> Some Probability Concepts and Sample Distributions [Lesson-1] [List-1] </h3>
              <h4><p>
                &#x1F393; <font color="gray">
                    The idea is to give a brief probability theory overview of the main results used in statistical inference.
                    This topic will conclude with a discussion of the main sampling distributions that are
                    that occur in this course.
               </font>
              </p></h4>
            </section></li>
            <li>
            <section>
                <h3> General Estimation Methods [Lesson-2] [List-2] </h3>
                <h4><p>
                    &#x1F393; <font color="gray">
                    In this part of the course we introduce some important methods to obtain estimators 
                    for population parameters. Bayesian estimators are discussed in detail to illustrate 
                    the different estimators that are allowed.
                    Maximum likelihood estimators (MLE) and the method of moments are briefly discussed. 
                    Although MLE is generally considered to be the most important estimation method, 
                    its asymptotic optimality properties are best illustrated in the part of 
                    the course dealing with large sample theory.</font> 
                </p></h4>
            </section></li>
            <li><section>
                <h3> Sufficient Statistics, Exponential Families, and Estimation [Lesson-3] [List-3]</h3>
                <h4><p> 
                    &#x1F393; <font color="gray">
                    A sufficient statistic is a function of the observed data and contains all the information 
                    about the model you are interested in. Furthermore, a complete sufficient statistic reduces 
                    the data the most without losing information. More importantly, according to Rao–Blackwell's 
                    and Lehmann–Scheff´s theorems, statistical inference procedures must be based on such a 
                    statistic for reasons of efficiency or optimality.</font>  
                </p></h4>
            </section></li>
            <li><section>
                <h3> Testing Hypotheses [Lesson-4] [List-4] </h3>
                <h4><p>
                    &#x1F393; <font color="gray">As you know. The Neyman–Pearson lemma provides the 
                        most powerful test of any size for a simple null hypothesis H0 against a simple alternative 
                        hypothesis H1. For one-parameter exponential models, such tests are uniformly most powerful (UMP) vs
                        one-sided alternatives (UMP).
                       For two-sided alternatives, a UMP test can be determined among all unbiased tests of a given size. 
                       Similarly, for exponential models with multiple parameters, unbiased UMP tests can be determined in 
                       the presence of nuisance parameters. For statistical models that are invariant under a set of 
                       transformations, all reasonable tests should be invariant under that set. In addition, the theory 
                       of UMP tests is developed under all invariant tests for linear models. </font>
                </p></h4>
            </section></li>
        <li><section>
                <h3> Consistency and Asymptotic Distributions of Statistics [Lesson-5] [List-5] </h3>
                <h4><p>
                    &#x1F393; <font color="gray"> 
                    The concepts of the theory of large samples are taken up again: almost sure convergence, 
                    convergence in probability and convergence in distribution. The consistency of estimators and 
                    their asymptotic distributions are discussed.
                 </font> 
                </p></h4>
        </section></li>
        <li><section>
            <h3> Large Sample Theory of Estimation in Parametric Models [Lesson-6] [List-6] </h3>
            <h4><p>
                &#x1F393; <font color="gray">
                This part deals with the asymptotic normality and optimality of the maximum likelihood estimator under 
                regularity conditions.
                It is shown that the lower bound of Cramér-Rao for the variance of unbiased estimators of parametric 
                functions is asymptotically achieved by the MLE.
             </font> 
            </p></h4>
    </section></li>
    <li><section>
        <h3> Tests in Parametric and Nonparametric Models [Lesson-7] [List-7]</h3>
        <h4><p>
            &#x1F393; <font color="gray">
                The asymptotic theory of tests in parametric and non-parametric models and their relative efficiency 
                are presented here. In particular, the livelihood ratio, the Wald test and the Chisquare test in 
                parametric models are derived. The non-parametric tests discussed include the two-sample rank tests 
                and the Kolmogorov–Smirnov tests.
         </font> 
        </p></h4>
</section></li>
    </ol>
    ......................................................................... 
    &#8658; IMPORTANT DATES     
    .........................................................................
    <ol start="1">
    <li><section> 
        <h3> First test: From Topic ?? to ??. Date: ??-??-2024.</h3> 
    </section></li>
    <li><section> 
        <h3> Second test: From Topic ?? to ??. Date: ??-??-2024.</h3> 
    </section></li>
    <li><section> 
        <h3> Third test: From Topic ?? to ??. Date: ??-??-2024. </h3>
    </section></li>
    </ol>
    .........................................................................
    ......................................................................... 
    &#8658; REFERENCES     
    .........................................................................
    <ol start="1">
    <li><section> 
        <h3>  George Casella and Roger L. Berger, Statistical Inference, Cengage Learning, 2021.</h3> 
    </section></li>
    <li><section> 
        <h3> Alexander M. Mood and Franklin A. Graybill, Introduction to the Theory of Statistics, 
            McGraw-Hill Book Company, New York, 1963.</h3> 
    </section></li>
    <li><section> 
        <h3> Nitis Mukhopadhyay, Probability and Statistical Inference, CRC Press, 2000. </h3>
    </section></li>
    <li><section> 
        <h3> Rabi Bhattacharya, Lizhen Lin and Victor Patrangenaru, A Course in Mathematical Statistics and Large Sample Theory, 
            CRC Press, 2016. </h3>
    </section></li>
    </ol>
    .........................................................................

</main>
</body>
</html>